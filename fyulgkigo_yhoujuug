############### STEP 1: Data import ###############
# Import data from MATLAB and save as Rdata
# Author: Florian Scharf, florian.scharf@uni-muenster.de and Andreas Widmann, widmann@uni-leipzig.de
# Copyright (c) 2021 Florian Scharf, University of MÃ¼nster and Andreas Widmann, University of Leipzig


## Empty workspace to start with a clean plate.
rm(list=ls())

## Check if necessary packages are installed and if not
# install them
if(!require(R.matlab)) install.packages("R.matlab")
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(psych)) install.packages("psych")
if(!require(MASS)) install.packages("MASS")
if(!require(GPArotation)) install.packages("GPArotation")

## Load packages
library(R.matlab)
library(ggplot2)
library(psych)
library(MASS)
library(GPArotation)


### Read participant averages 
avrdata = readMat("Z:/Bureau/data_analysis/data/Data_R.mat")


### Combine the data set into a "nice" and labeled R dataframe
colnames(avrdata$data) = paste0("erp_", 1:1000)
colnames(avrdata$dataIdx) = c("group", "subj", "cond", "chan")

erpdata = data.frame(cbind(avrdata$dataIdx, avrdata$data))
erpdata$group = factor(erpdata$group, labels = c("ty", "at"))
erpdata$subj = factor(erpdata$subj)
erpdata$cond = factor(erpdata$cond, labels = c("con", "dev", "fam", "omi", "pom", "std", "moy"))
erpdata$chan = factor(erpdata$chan)
###

### We also save some data characteristics 
fs = 1000 # sampling rate
xmin = -0.1 # baseline
pnts = 500 # epoch duration
lat = (1:pnts - 1) / fs + xmin
###

### Save as Rdata-File
save(erpdata, fs, xmin, pnts, lat, file = "Z:/Bureau/data_analysis/data/erpdata.Rdata")












## Select group to be analyzed
# ty = typical; at = atypical
group = "ty"
# Please remember that this script needs to be run once for each group.


## Prepare data set 
# Select a subset of the data based on the chosen group
# delete all variable except for the sampling points (necessary for the pca functions)
age_group_data = as.matrix(erpdata[erpdata$group == group, -c(1:4)])

############### 2a:Determination of the Number of Factors ############### 

## Compute correlation matrix of the sampling points 
# to be used for the Empirical Kaiser Criterion (EKC)
# Attention: R should ONLY contain correlations between the variables
# which shall enter the pca. Please make sure to remove any indicator variables
# such as participant or condition ID before computing R!
R = cor(age_group_data)

#Fonction EKC

EKC <- function (R, N = NA, use = c("pairwise.complete.obs", "all.obs", 
                             "complete.obs", "everything", "na.or.complete"), cor_method = c("pearson", 
                                                                                             "spearman", "kendall")) 
{

  p <- ncol(R)
  lambda <- eigen(R, symmetric = TRUE, only.values = TRUE)$values
  refs <- vector("double", p)
  for (i in seq_len(p)) {
    refs[i] <- max(((1 + sqrt(p/N))^2) * (p - sum(refs))/(p - 
                                                            i + 1), 1)
  }
  out <- list(eigenvalues = lambda, n_factors = which(lambda <= 
                                                        refs)[1] - 1, references = refs, settings = list(use = use, 
                                                                                                         cor_method = cor_method, N = N))
  class(out) <- "EKC"
  return(out)
}


## Determine number of factors using 

res_ekc = EKC(R, N = nrow(age_group_data))

## Save the resulting number of factors
nFac = res_ekc$n_factors

## Print the number of factors
nFac

## Plot the variance explained by each factor in the initial
# solution, we cut the solution at 40 factors just to make the
# cut point more visible.
plot(1:ncol(age_group_data), res_ekc$eigenvalues,
     xlab = "Factor", ylab = "Variance Explained",
     main = ifelse(group == "ty", "Typical", "Atypical"),
     xlim = c(0,40), pch = 16,
     col = (res_ekc$references <= res_ekc$eigenvalues) + 1)
lines(1:ncol(age_group_data), res_ekc$references, lty = 2, lwd = 3,
      col = "blue")

abline(v = nFac)
text(x = nFac, y = 100, pos = 2, cex = 0.8,  
     labels = paste0("Number of Factors\nto be extracted: ", nFac))

### Optional code for 
## Horn's parallel test as implemented by Dien's ERP PCA toolkit
# Eigen values of the data
S = cov(age_group_data)
eigenData = eigen(S)$values

# Generate uncorrelated random data with the same
# standard deviation as the original data
randData = matrix(rnorm(n = dim(age_group_data)[1] * dim(age_group_data)[2], 
                        sd = sqrt(diag(S))), 
                  nrow = dim(age_group_data)[1])

# Covariance matrix and eigen values of the random data
Srand = cov(randData)
eigenRandData = eigen(Srand)$values

# How many eigen values of the real data are
# above their random counterparts?
# sum(eigenRandData < eigenData) 
min(which(eigenRandData >= eigenData)) - 1
# Uncomment and replace nFac if you want to overwrite
# the EKC result and use a different number of factors
# nFac <- 9999

## A basic plot of parallel analysis
# we only plot the first 50 eigen values
# to keep the figure comprehensible
plot(eigenData[1:50],
     xlab = "Factors",
     ylab = "Eigen Values")
lines(eigenRandData, col = "red")

############### 
fa_simplified function

#This is just the psych::fa-Function but without chisq-stats to improve speed.

fa_simplified = function (r, nfactors = 1, n.obs = NA, rotate = "oblimin", scores = "tenBerge", 
               residuals = FALSE, SMC = TRUE, covar = FALSE, missing = FALSE, 
               impute = "median", min.err = 0.001, max.iter = 50, symmetric = TRUE, 
               warnings = TRUE, fm = "minres", alpha = 0.1, oblique.scores = FALSE, 
               np.obs = NULL, use = "pairwise", cor = "cor", weight = NULL, 
               ...) 
{
  cl <- match.call()
  control <- NULL
  "fit.residuals" <- function(Psi, S, nf, S.inv, fm) {
    diag(S) <- 1 - Psi
    if (!is.null(S.inv)) 
      sd.inv <- diag(1/diag(S.inv))
    eigens <- eigen(S)
    eigens$values[eigens$values < .Machine$double.eps] <- 100 * 
      .Machine$double.eps
    if (nf > 1) {
      loadings <- eigens$vectors[, 1:nf] %*% diag(sqrt(eigens$values[1:nf]))
    }
    else {
      loadings <- eigens$vectors[, 1] * sqrt(eigens$values[1])
    }
    model <- loadings %*% t(loadings)
    switch(fm, wls = {
      residual <- sd.inv %*% (S - model)^2 %*% sd.inv
    }, gls = {
      residual <- (S.inv %*% (S - model))^2
    }, uls = {
      residual <- (S - model)^2
    }, minres = {
      residual <- (S - model)^2
      diag(residual) <- 0
    }, minchi = {
      residual <- (S - model)^2
      residual <- residual * np.obs
      diag(residual) <- 0
    })
    error <- sum(residual)
  }
  "fit" <- function(S, nf, fm, covar) {
    S.smc <- smc(S, covar)
    if ((fm == "wls") | (fm == "gls")) {
      S.inv <- solve(S)
    }
    else {
      S.inv <- NULL
    }
    if (!covar && (sum(S.smc) == nf) && (nf > 1)) {
      start <- rep(0.5, nf)
    }
    else {
      start <- diag(S) - S.smc
    }
    if (fm == "ml" || fm == "mle") {
      res <- optim(start, FAfn, FAgr, method = "L-BFGS-B", 
                   lower = 0.005, upper = 1, control = c(list(fnscale = 1, 
                                                              parscale = rep(0.01, length(start))), control), 
                   nf = nf, S = S)
    }
    else {
      res <- optim(start, fit.residuals, gr = FAgr.minres, 
                   method = "L-BFGS-B", lower = 0.005, upper = 1, 
                   control = c(list(fnscale = 1, parscale = rep(0.01, 
                                                                length(start)))), nf = nf, S = S, S.inv = S.inv, 
                   fm = fm)
    }
    if ((fm == "wls") | (fm == "gls")) {
      Lambda <- FAout.wls(res$par, S, nf)
    }
    else {
      Lambda <- FAout(res$par, S, nf)
    }
    result <- list(loadings = Lambda, res = res, S = S)
  }
  FAfn <- function(Psi, S, nf) {
    sc <- diag(1/sqrt(Psi))
    Sstar <- sc %*% S %*% sc
    E <- eigen(Sstar, symmetric = TRUE, only.values = TRUE)
    e <- E$values[-(1:nf)]
    e <- sum(log(e) - e) - nf + nrow(S)
    -e
  }
  FAgr <- function(Psi, S, nf) {
    sc <- diag(1/sqrt(Psi))
    Sstar <- sc %*% S %*% sc
    E <- eigen(Sstar, symmetric = TRUE)
    L <- E$vectors[, 1:nf, drop = FALSE]
    load <- L %*% diag(sqrt(pmax(E$values[1:nf] - 1, 0)), 
                       nf)
    load <- diag(sqrt(Psi)) %*% load
    g <- load %*% t(load) + diag(Psi) - S
    diag(g)/Psi^2
  }
  FAgr.minres <- function(Psi, S, nf, S.inv, fm) {
    sc <- diag(1/sqrt(Psi))
    Sstar <- sc %*% S %*% sc
    E <- eigen(Sstar, symmetric = TRUE)
    L <- E$vectors[, 1:nf, drop = FALSE]
    load <- L %*% diag(sqrt(pmax(E$values[1:nf] - 1, 0)), 
                       nf)
    load <- diag(sqrt(Psi)) %*% load
    g <- load %*% t(load) + diag(Psi) - S
    if (fm == "minchi") {
      g <- g * np.obs
    }
    diag(g)/Psi^2
  }
  FAout <- function(Psi, S, q) {
    sc <- diag(1/sqrt(Psi))
    Sstar <- sc %*% S %*% sc
    E <- eigen(Sstar, symmetric = TRUE)
    L <- E$vectors[, 1L:q, drop = FALSE]
    load <- L %*% diag(sqrt(pmax(E$values[1L:q] - 1, 0)), 
                       q)
    diag(sqrt(Psi)) %*% load
  }
  FAout.wls <- function(Psi, S, q) {
    diag(S) <- 1 - Psi
    E <- eigen(S, symmetric = TRUE)
    L <- E$vectors[, 1L:q, drop = FALSE] %*% diag(sqrt(E$values[1L:q, 
                                                                drop = FALSE]), q)
    return(L)
  }
  "MRFA" <- function(S, nf) {
    com.glb <- glb.algebraic(S)
    L <- FAout.wls(1 - com.glb$solution, S, nf)
    h2 <- com.glb$solution
    result <- list(loadings = L, communality = h2)
  }
  if (fm == "mle" || fm == "MLE" || fm == "ML") 
    fm <- "ml"
  if (!any(fm %in% (c("pa", "minrank", "wls", "gls", "minres", 
                      "minchi", "uls", "ml", "mle")))) {
    message("factor method not specified correctly, minimum residual (unweighted least squares  used")
    fm <- "minres"
  }
  x.matrix <- r
  n <- dim(r)[2]
  if (n != dim(r)[1]) {
    matrix.input <- FALSE
    n.obs <- dim(r)[1]
    if (missing) {
      x.matrix <- as.matrix(x.matrix)
      miss <- which(is.na(x.matrix), arr.ind = TRUE)
      if (impute == "mean") {
        item.means <- colMeans(x.matrix, na.rm = TRUE)
        x.matrix[miss] <- item.means[miss[, 2]]
      }
      else {
        item.med <- apply(x.matrix, 2, median, na.rm = TRUE)
        x.matrix[miss] <- item.med[miss[, 2]]
      }
    }
    np.obs <- pairwiseCount(r)
    if (covar) {
      cor <- "cov"
    }
    if (!is.null(weight)) 
      cor <- "wtd"
    switch(cor, cor = {
      r <- cor(r, use = use)
    }, cov = {
      r <- cov(r, use = use)
      covar <- TRUE
    }, wtd = {
      r <- cor.wt(r, w = weight)$r
    }, tet = {
      r <- tetrachoric(r)$rho
    }, poly = {
      r <- polychoric(r)$rho
    }, mixed = {
      r <- mixed.cor(r, use = use)$rho
    }, Yuleb = {
      r <- YuleCor(r, bonett = TRUE)$rho
    }, YuleQ = {
      r <- YuleCor(r, 1)$rho
    }, YuleY = {
      r <- YuleCor(r, 0.5)$rho
    })
  }
  else {
    matrix.input <- TRUE
    if (fm == "minchi") {
      if (is.null(np.obs)) {
        fm <- "minres"
        message("factor method minchi does not make sense unless we know the sample size, minres used instead")
      }
    }
    if (is.na(n.obs) && !is.null(np.obs)) 
      n.obs <- max(as.vector(np.obs))
    if (!is.matrix(r)) {
      r <- as.matrix(r)
    }
    if (!covar) {
      r <- cov2cor(r)
    }
  }
  if (!residuals) {
    result <- list(values = c(rep(0, n)), rotation = rotate, 
                   n.obs = n.obs, np.obs = np.obs, communality = c(rep(0, 
                                                                       n)), loadings = matrix(rep(0, n * n), ncol = n), 
                   fit = 0)
  }
  else {
    result <- list(values = c(rep(0, n)), rotation = rotate, 
                   n.obs = n.obs, np.obs = np.obs, communality = c(rep(0, 
                                                                       n)), loadings = matrix(rep(0, n * n), ncol = n), 
                   residual = matrix(rep(0, n * n), ncol = n), fit = 0, 
                   r = r)
  }
  if (is.null(SMC)) 
    SMC = TRUE
  r.mat <- r
  Phi <- NULL
  colnames(r.mat) <- rownames(r.mat) <- colnames(r)
  if (any(is.na(r))) {
    bad <- TRUE
    tempr <- r
    wcl <- NULL
    while (bad) {
      wc <- table(which(is.na(tempr), arr.ind = TRUE))
      wcl <- c(wcl, as.numeric(names(which(wc == max(wc)))))
      tempr <- r[-wcl, -wcl]
      if (any(is.na(tempr))) {
        bad <- TRUE
      }
      else {
        bad <- FALSE
      }
    }
    cat("\nLikely variables with missing values are ", colnames(r)[wcl], 
        " \n")
    stop("I am sorry: missing values (NAs) in the correlation matrix do not allow me to continue.\nPlease drop those variables and try again.")
  }
  if (is.logical(SMC)) {
    if (SMC) {
      if (nfactors <= n) {
        diag(r.mat) <- smc(r, covar = covar)
      }
      else {
        if (warnings) {
          message("In fa, too many factors requested for this number of variables to use SMC for communality estimates, 1s are used instead")
        }
      }
    }
    else {
      diag(r.mat) <- 1
    }
  }
  else {
    diag(r.mat) <- SMC
  }
  orig <- diag(r)
  comm <- sum(diag(r.mat))
  err <- comm
  i <- 1
  comm.list <- list()
  if (fm == "pa") {
    e.values <- eigen(r, symmetric = symmetric)$values
    while (err > min.err) {
      eigens <- eigen(r.mat, symmetric = symmetric)
      if (nfactors > 1) {
        loadings <- eigens$vectors[, 1:nfactors] %*% 
          diag(sqrt(eigens$values[1:nfactors]))
      }
      else {
        loadings <- eigens$vectors[, 1] * sqrt(eigens$values[1])
      }
      model <- loadings %*% t(loadings)
      new <- diag(model)
      comm1 <- sum(new)
      diag(r.mat) <- new
      err <- abs(comm - comm1)
      if (is.na(err)) {
        warning("imaginary eigen value condition encountered in fa\n Try again with SMC=FALSE \n exiting fa")
        break
      }
      comm <- comm1
      comm.list[[i]] <- comm1
      i <- i + 1
      if (i > max.iter) {
        if (warnings) {
          message("maximum iteration exceeded")
        }
        err <- 0
      }
    }
    eigens <- eigens$values
  }
  if (fm == "minrank") {
    mrfa <- MRFA(r, nfactors)
    loadings <- mrfa$loadings
    model <- loadings %*% t(loadings)
    e.values <- eigen(r)$values
    S <- r
    diag(S) <- diag(model)
    eigens <- eigen(S)$values
  }
  if ((fm == "wls") | (fm == "minres") | (fm == "minchi") | 
      (fm == "gls") | (fm == "uls") | (fm == "ml") | (fm == 
                                                      "mle")) {
    uls <- fit(r, nfactors, fm, covar = covar)
    e.values <- eigen(r)$values
    result.res <- uls$res
    loadings <- uls$loadings
    model <- loadings %*% t(loadings)
    S <- r
    diag(S) <- diag(model)
    eigens <- eigen(S)$values
  }
  if (!is.double(loadings)) {
    warning("the matrix has produced imaginary results -- proceed with caution")
    loadings <- matrix(as.double(loadings), ncol = nfactors)
  }
  if (nfactors > 1) {
    sign.tot <- vector(mode = "numeric", length = nfactors)
    sign.tot <- sign(colSums(loadings))
    sign.tot[sign.tot == 0] <- 1
    loadings <- loadings %*% diag(sign.tot)
  }
  else {
    if (sum(loadings) < 0) {
      loadings <- -as.matrix(loadings)
    }
    else {
      loadings <- as.matrix(loadings)
    }
    colnames(loadings) <- "MR1"
  }
  switch(fm, wls = {
    colnames(loadings) <- paste("WLS", 1:nfactors, sep = "")
  }, pa = {
    colnames(loadings) <- paste("PA", 1:nfactors, sep = "")
  }, gls = {
    colnames(loadings) <- paste("GLS", 1:nfactors, sep = "")
  }, ml = {
    colnames(loadings) <- paste("ML", 1:nfactors, sep = "")
  }, minres = {
    colnames(loadings) <- paste("MR", 1:nfactors, sep = "")
  }, minrank = {
    colnames(loadings) <- paste("MRFA", 1:nfactors, sep = "")
  }, minchi = {
    colnames(loadings) <- paste("MC", 1:nfactors, sep = "")
  })
  rownames(loadings) <- rownames(r)
  loadings[loadings == 0] <- 10^-15
  model <- loadings %*% t(loadings)
  f.loadings <- loadings
  rot.mat <- NULL
  if (rotate != "none") {
    if (nfactors > 1) {
      if (rotate == "varimax" | rotate == "Varimax" | rotate == 
          "quartimax" | rotate == "bentlerT" | rotate == 
          "geominT" | rotate == "targetT" | rotate == "bifactor" | 
          rotate == "TargetT" | rotate == "equamax" | rotate == 
          "varimin" | rotate == "specialT" | rotate == 
          "Promax" | rotate == "promax" | rotate == "cluster" | 
          rotate == "biquartimin" | rotate == "TargetQ" | 
          rotate == "specialQ") {
        Phi <- NULL
        switch(rotate, varimax = {
          rotated <- stats::varimax(loadings)
          loadings <- rotated$loadings
          rot.mat <- rotated$rotmat
        }, Varimax = {
          if (!requireNamespace("GPArotation")) {
            stop("I am sorry, to do this rotation requires the GPArotation package to be installed")
          }
          rotated <- GPArotation::Varimax(loadings, ...)
          loadings <- rotated$loadings
          rot.mat <- t(solve(rotated$Th))
        }, quartimax = {
          if (!requireNamespace("GPArotation")) {
            stop("I am sorry, to do this rotation requires the GPArotation package to be installed")
          }
          rotated <- GPArotation::quartimax(loadings, 
                                            ...)
          loadings <- rotated$loadings
          rot.mat <- t(solve(rotated$Th))
        }, bentlerT = {
          if (!requireNamespace("GPArotation")) {
            stop("I am sorry, to do this rotation requires the GPArotation package to be installed")
          }
          rotated <- GPArotation::bentlerT(loadings, 
                                           ...)
          loadings <- rotated$loadings
          rot.mat <- t(solve(rotated$Th))
        }, geominT = {
          if (!requireNamespace("GPArotation")) {
            stop("I am sorry, to do this rotation requires the GPArotation package to be installed")
          }
          rotated <- GPArotation::geominT(loadings, ...)
          loadings <- rotated$loadings
          rot.mat <- t(solve(rotated$Th))
        }, targetT = {
          if (!requireNamespace("GPArotation")) {
            stop("I am sorry, to do this rotation requires the GPArotation package to be installed")
          }
          rotated <- GPArotation::targetT(loadings, Tmat = diag(ncol(loadings)), 
                                          ...)
          loadings <- rotated$loadings
          rot.mat <- t(solve(rotated$Th))
        }, bifactor = {
          rot <- bifactor(loadings, ...)
          loadings <- rot$loadings
          rot.mat <- t(solve(rot$Th))
        }, TargetT = {
          if (!requireNamespace("GPArotation")) {
            stop("I am sorry, to do this rotation requires the GPArotation package to be installed")
          }
          rot <- GPArotation::targetT(loadings, Tmat = diag(ncol(loadings)), 
                                      ...)
          loadings <- rot$loadings
          rot.mat <- t(solve(rot$Th))
        }, equamax = {
          rot <- equamax(loadings, ...)
          loadings <- rot$loadings
          rot.mat <- t(solve(rot$Th))
        }, varimin = {
          rot <- varimin(loadings, ...)
          loadings <- rot$loadings
          rot.mat <- t(solve(rot$Th))
        }, specialT = {
          rot <- specialT(loadings, ...)
          loadings <- rot$loadings
          rot.mat <- t(solve(rot$Th))
        }, Promax = {
          pro <- Promax(loadings, ...)
          loadings <- pro$loadings
          Phi <- pro$Phi
          rot.mat <- pro$rotmat
        }, promax = {
          pro <- stats::promax(loadings, ...)
          loadings <- pro$loadings
          rot.mat <- pro$rotmat
          ui <- solve(rot.mat)
          Phi <- cov2cor(ui %*% t(ui))
        }, cluster = {
          loadings <- varimax(loadings, ...)$loadings
          pro <- target.rot(loadings)
          loadings <- pro$loadings
          Phi <- pro$Phi
          rot.mat <- pro$rotmat
        }, biquartimin = {
          ob <- biquartimin(loadings, ...)
          loadings <- ob$loadings
          Phi <- ob$Phi
          rot.mat <- t(solve(ob$Th))
        }, TargetQ = {
          ob <- TargetQ(loadings, ...)
          loadings <- ob$loadings
          Phi <- ob$Phi
          rot.mat <- t(solve(ob$Th))
        }, specialQ = {
          ob <- specialQ(loadings, ...)
          loadings <- ob$loadings
          Phi <- ob$Phi
          rot.mat <- t(solve(pro$Th))
        })
      }
      else {
        if (rotate == "oblimin" | rotate == "quartimin" | 
            rotate == "simplimax" | rotate == "geominQ" | 
            rotate == "bentlerQ" | rotate == "targetQ") {
          if (!requireNamespace("GPArotation")) {
            warning("I am sorry, to do these rotations requires the GPArotation package to be installed")
            Phi <- NULL
          }
          else {
            ob <- try(do.call(getFromNamespace(rotate, 
                                               "GPArotation"), list(loadings, ...)))
            if (class(ob) == as.character("try-error")) {
              warning("The requested transformaton failed, Promax was used instead as an oblique transformation")
              ob <- Promax(loadings)
            }
            loadings <- ob$loadings
            Phi <- ob$Phi
            rot.mat <- t(solve(ob$Th))
          }
        }
        else {
          message("Specified rotation not found, rotate='none' used")
        }
      }
    }
  }
  signed <- sign(colSums(loadings))
  signed[signed == 0] <- 1
  loadings <- loadings %*% diag(signed)
  if (!is.null(Phi)) {
    Phi <- diag(signed) %*% Phi %*% diag(signed)
  }
  switch(fm, wls = {
    colnames(loadings) <- paste("WLS", 1:nfactors, sep = "")
  }, pa = {
    colnames(loadings) <- paste("PA", 1:nfactors, sep = "")
  }, gls = {
    colnames(loadings) <- paste("GLS", 1:nfactors, sep = "")
  }, ml = {
    colnames(loadings) <- paste("ML", 1:nfactors, sep = "")
  }, minres = {
    colnames(loadings) <- paste("MR", 1:nfactors, sep = "")
  }, minrank = {
    colnames(loadings) <- paste("MRFA", 1:nfactors, sep = "")
  }, uls = {
    colnames(loadings) <- paste("ULS", 1:nfactors, sep = "")
  }, minchi = {
    colnames(loadings) <- paste("MC", 1:nfactors, sep = "")
  })
  if (nfactors > 1) {
    ev.rotated <- diag(t(loadings) %*% loadings)
    ev.order <- order(ev.rotated, decreasing = TRUE)
    loadings <- loadings[, ev.order]
  }
  rownames(loadings) <- colnames(r)
  if (!is.null(Phi)) {
    Phi <- Phi[ev.order, ev.order]
  }
  class(loadings) <- "loadings"
  if (nfactors < 1) 
    nfactors <- n
  if (max(abs(loadings) > 1) && !covar) 
    warning(" A Heywood case was detected.  Examine the loadings carefully.")
  #result <- factor.stats(r, loadings, Phi, n.obs = n.obs, np.obs = np.obs, 
  #                       alpha = alpha)
  result$rotation <- rotate
  result$communality <- diag(model)
  if (fm == "minrank") {
    result$communalities <- mrfa$communality
  }
  else {
    if (fm == "pa") {
      result$communalities <- comm1
    }
    else {
      result$communalities <- 1 - result.res$par
    }
  }
  result$uniquenesses <- diag(r - model)
  result$values <- eigens
  result$e.values <- e.values
  result$loadings <- loadings
  result$model <- model
  result$fm <- fm
  result$rot.mat <- rot.mat
  if (!is.null(Phi)) {
    colnames(Phi) <- rownames(Phi) <- colnames(loadings)
    result$Phi <- Phi
    Structure <- loadings %*% Phi
  }
  else {
    Structure <- loadings
  }
  class(Structure) <- "loadings"
  result$Structure <- Structure
  if (fm == "pa") 
    result$communality.iterations <- unlist(comm.list)
  if (oblique.scores) {
    result$scores <- factor.scores(x.matrix, f = loadings, 
                                   Phi = Phi, method = scores)
  }
  else {
#    result$scores <- factor.scores(x.matrix, f = Structure, 
#                                   method = scores)
  }
  result$weights <- result$scores$weights
  result$scores <- result$scores$scores
  if (!is.null(result$scores)) 
    colnames(result$scores) <- colnames(loadings)
  result$factors <- nfactors
  result$r <- r
  result$np.obs <- np.obs
  result$fn <- "fa"
  result$fm <- fm
  result$Call <- cl
  class(result) <- c("psych", "fa")
  return(result)
}










############### 2b: Estimation of unrotated factor loadings ############### 

## Estimate unrotated factor loadings
# This function is adapted from the package psych. We slightly modified it to 
# improve estimation speed. The original function tries to compute a number of
# fit indices which are not relevant for our purposes. However, this computation
# is very slow for data sets as large as ours and it often results in irrelevant 
# warnings about non-convergence of the fit index estimation. Otherwise, the
# function is not changed.
source("tools/fa_simplified.R")

# The argument covar = TRUE is set in order to produce an unstandardized 
# solution as intended.
pcaFit = fa_simplified(r = age_group_data, nfactors = nFac, rotate = "none", covar = TRUE)

# We add a number of relevant descriptive statistics to the fit object
# which we use again in later steps.

S = cov(age_group_data)
Sinv = ginv(S)
Rinv = ginv(R)
Var = diag(S)
varSD = sqrt(Var)

pcaFit$S = S # Sample Covariance Matrix
pcaFit$Sinv = Sinv # (Moore-Penrose) Inverse of the Sample Covariance Matrix
pcaFit$R = R # Sample Correlation Matrox
pcaFit$Rinv = Rinv # (Moore-Penrose) Inverse of the Sample Covariance Matrix
pcaFit$varSD = varSD # SDs of the Sampling Points
pcaFit$Var = Var # Variances of the Sampling Points
pcaFit$group = group

# Finally, we save the unrotated solutions in a separate file for the
# next analysis steps. 
# The file name is automatically adjusted to include group name
# and number of factors.
save(pcaFit, file = paste0("Z:/Bureau/data_analysis/data/pcafit_", group, nFac, ".Rdata"))
###############

geomin function

geominQ_multstart = function(A, normalize = FALSE, delta = 0.01, Tmat = diag(ncol(A)), start.values = 30, rand.start = TRUE,
                             maxit = 10000, eps = 1e-5, method = "geomin"){
  
  
  if (rand.start){ # if you are supposed to use random starts
    
    # Use as many starting values as indicated
    mrRot <- lapply(1:start.values, function(iStart){
      # Generate a random starting rotation
      Tmat <- Random.Start(ncol(A))
      # Tell the user where you are in the process
      print(paste0("Random start number: ", iStart))
      
      # Apply gradient projection - parameters are passed from the function 
      rotFit <- GPFoblq(A = A, method = method, normalize = normalize, Tmat = Tmat,
                           eps = eps, maxit = maxit, methodArgs = list(delta = delta))
      # Remember the results in each round
      list(criterion = ifelse(is.null(rotFit$Table), NA, min(rotFit$Table[,2])), rotFit = rotFit)
    })
    
    # Which of the random starting values resulted in the best value
    # of the criterion?
    bestStart <- which.min(lapply(mrRot, function(x) x$criterion))
    
    # If bestStart is empty, notify the user about it.
    if (length(bestStart) == 0) stop("No best solution found, perhaps no rotation converged? Try increasing maxit or reducing eps!")
    # If the function received reasonable input, non-convergence is basically the only
    # thing that can go wrong. 
    
    # return the best rotation results as the result
    rotFit <- mrRot[[bestStart]]$rotFit
    
  }
  else { # else and rotate: start from a generic starting matrix
    # Apply gradient projection - parameters are passed from the function 
     rotFit = GPFoblq(A = A, method = method, normalize = normalize, Tmat = Tmat,
                        eps = eps, maxit = maxit, methodArgs = list(delta = delta))
  }
  
  return(rotFit)
}

#######################

pcaFit$loadings = pcaFit$loadings / pcaFit$varSD

## Use Geomin rotation with 30 random start values.
# Please be aware that this function can take a while. 
# This is a custom function which implements a procedure 
# available from the package GPArotation. 
# Please see tools/geominQ_multstart.R for details
rotFit <- geominQ_multstart(A = pcaFit$loadings,  # unrotated loadings
                            delta = 0.01,     # rotation parameter (geomin epsilon)
                            # Note: We decided to name all parameters consistently with
                            # the GPArotation package despite its deviation from the
                            # conventional naming epsilon for this parameter.
                            normalize = F,     # No additional standardization
                            rand.start = T,    # Use multiple random starts
                            start.values = 30, # Number of random starts
                            maxit = 50000,     # Number of iterations 
                            # Note: After this number of iterations, the
                            # function stops trying to estimate the parameters
                            # from this random starting values.
                            eps = 1e-5)        # Level of accuracy to determine convergence
                            # Note: This means that the rotation is declared successfully 
                            # converged when the criterion changes less than eps between
                            # two iterations. We recommend against using lower values 
                            # (this is the GPArotation default) since it can prevent
                            # the algorithm from converging.
                            # You can choose a higher values (e.g., 1e-3) if the
                            # rotation takes too long but we recommend using the default
                            # value if you are seriously interested in the results.

### IMPORTANT: Please increase maxit in case you encounter many warnings
### regarding non-converged solutions.

#### OPTIONAL: If you want to apply a Promax rotation.
# Promax does not rely on a general gradient projection algorithm
# and does not require random starts (but it tends to conflate factors
# earlier than Geomin):

# We set the rotation parameter to 3, mimicking the behavior of Dien's ERP
# PCA Toolkit and the commercial software SAS, 
# because this value is well-justified by simulation studies.
# Choosing a value of 2 will allow for solutions with higher temporal overlap,
# choosing higher values will increase the preference for too simple structures.

# rotFit <- Promax(pcaFit$loadings, m = 3, normalize = TRUE)

## Transfer variances and standard deviations into the new fit object.
rotFit$varSD = pcaFit$varSD
rotFit$Var = pcaFit$Var
rotFit$group = pcaFit$group

##  Sort factors by variance explained

# Compute unstandardized loadings
L <- rotFit$loadings * rotFit$varSD
# Compute proportion of variance explained by each factor
facVar <- diag(rotFit$Phi %*% t(L) %*% (L)) / sum(diag(pcaFit$S)) 


# Return indices of the factors ordered by the variance explained
alignment = order(facVar, decreasing = TRUE)
# reorder columns of factor loadings matrix in descending order of variance
# explained
rotFit$loadings = rotFit$loadings[, alignment]
# reorder factor correlation matrix as well
rotFit$Phi = rotFit$Phi[alignment, alignment]

## Flip mostly negative factor loadings
flip = sign(colSums(rotFit$loadings)) # flip now contains -1 and 1, we want to turn all the -1s
rotFit$loadings = rotFit$loadings %*% diag(flip) # post multiplying with this turns the factor loadings
rotFit$Phi = diag(flip) %*%  rotFit$Phi %*% diag(flip) # turning factor correlations

###############

















############### 2c: Estimation of factor scores ###############

# Load raw data again (because they are necessary for factor scoring)
load("Z:/Bureau/data_analysis/data/erpdata.Rdata")

# Again, need to delete columns not containing data from sampling points
age_group_data = as.matrix(erpdata[erpdata$group == pcaFit$group, -c(1:4)])


## Estimate factor scores
# The formula is taken from DiStefano et al. (2009; Appendix 2).
# Note that "age_group_data" contains the unstandardized ERP individual averages 
# of the respective group.
# Therefore, the factor scores are not centered.
FacScr = age_group_data %*% pcaFit$Sinv %*% (rotFit$loadings * rotFit$varSD) %*% rotFit$Phi

# Rinv ... generalized inverse of the correlation matrix of the sampling points
# rotFit$loadings ... standardized factor loadings
# rotFit$Phi ... factor correlation matrix

# rename columns because they are factors now
colnames(FacScr) = paste0("MR", 1:pcaFit$factors)

# Show descriptive statistics for the factor scores
psych::describe(FacScr)

## We save the factor scores in a separate object and "reunite" them
# with the indicator variables describing participant, condition and 
# electrode site
scores = data.frame(erpdata[erpdata$group == pcaFit$group, 1:4], FacScr)

## We save all estimated objects for later steps.
save(pcaFit, rotFit, scores, file = paste0("Z:/Bureau/data_analysis/data/rotfit_", pcaFit$group, pcaFit$factors, "_geomin0.01.Rdata"))


## Export to MATLAB
# This is optional. If you prefer to work in MATLAB for visualization purposes,
# this is how you export the results to a mat-File.
library(R.matlab)
writeMat(paste0("Z:/Bureau/data_analysis/data/rotfit_", pcaFit$group, pcaFit$factors, "_geomin0.01.mat"), loadings = unclass(rotFit$loadings), phi = unclass(rotFit$Phi), scores = FacScr, varSD = unclass(pcaFit$varSD))














if(!require(ggplot2)) install.packages("ggplot2")
if(!require(RColorBrewer)) install.packages("RColorBrewer")
if(!require(reshape)) install.packages("reshape")
if(!require(xlsx)) install.packages("xlsx")
library(ggplot2)
library(RColorBrewer)
library(reshape)
library(xlsx)

load("Z:/Bureau/data_analysis/data/erpdata.Rdata")


















######### Plot the Time Courses (i.e., factor loadings) #########

for (iFile in c("Z:/Bureau/data_analysis/data/rotfit_ty13_geomin0.01.Rdata",
                "Z:/Bureau/data_analysis/data//rotfit_at23_geomin0.01.Rdata")){
  
  ## Load the results file from the previous script.
  # Please remember that this script needs to be run once for each group.
  load(iFile)
  
  ## Create a separate data object containing the latency (for the x-axis)
  # and the unstandardized loadings.
  loadings = data.frame(lat, Factor = unclass(rotFit$loadings) * rotFit$varSD)
  
  ## Reshape the data set into wide format (for the plotting function)
  loadings = melt(loadings, id.vars = "lat", variable_name = "Factor")
  
  ## Mark the factors which shall be highlighted in the loading figure
  # We highlight factors in the time range of interest.
  
  
  ### This marks the factors that should be highlighted in the data.
  if (rotFit$group == "ty"){
    loadings$highlight <- factor(ifelse(loadings$Factor %in% paste0("Factor.", c(3:6)),
                                        yes = 1,
                                        no = 0))
  } else if (rotFit$group == "at") {
    loadings$highlight <- factor(ifelse(loadings$Factor %in% paste0("Factor.", c(3:7)),
                                        yes = 1,
                                        no = 0))
  } else {
    stop("Unknown group.")
  }
  
  factorColors = colorRampPalette(brewer.pal(12, "Paired"))(pcaFit$factors)
  
  ggplot(data = loadings, aes(x = lat, y = value, color = Factor, size = highlight)) +
    geom_line() + 
    scale_size_manual(values = c(0.7,1.5)) +
    guides(size = "none", color = guide_legend(ncol = 2)) +
    ylim(-1,5) + 
    xlab("Time [s]") +
    ylab("Unstandardized Loadings") +
    labs(title = ifelse(pcaFit$group == "ty", "Adult PCA Geomin (0.01)", "Child PCA Geomin (0.01)")) + 
    # theme_classic() +
    scale_color_manual(values = factorColors) 
  
  ggsave(paste0("Z:/Bureau/data_analysis/data/rotfit3333333333_", pcaFit$group, pcaFit$factors, "_geomin0.01.pdf"), device = "pdf",
         width = 6, height = 3.5) 
}




























































































































################  Match factors #################

## load both PCA results to match factors 
load("Z:/Bureau/data_analysis/data/rotfit_ty13_geomin0.01.Rdata")
rotFit -> rotFit_childPCA
scores -> scores_childPCA
load("Z:/Bureau/data_analysis/data/rotfit_at23_geomin0.01.Rdata")
scores -> scores_adultPCA
rotFit -> rotFit_adultPCA

## compute average factor scores across participants
avr_scr_adPCA  <- aggregate(. ~ cond + chan, data = scores_adultPCA, FUN = mean)
avr_scr_chPCA  <- aggregate(. ~ cond + chan, data = scores_childPCA, FUN = mean)

####### Match time courses and topographies #######
## Correlation of factor loadings
# The rows are factors from the adultPCA, the columns from the childPCA.
# Higher correlations indicate higher similarity.
Rloadings <- cor(rotFit_adultPCA$loadings, rotFit_childPCA$loadings)

## Correlation of factor scores 
# Note: The excluded columns contain index variables for electrode sites etc.
# and need to be removed for the correlations to be meaningful.
Rtopo_sta <- cor(avr_scr_adPCA[avr_scr_adPCA$cond == "sta",-c(1:4)],
                 avr_scr_chPCA[avr_scr_chPCA$cond == "sta",-c(1:4)])

Rtopo_nov <- cor(avr_scr_adPCA[avr_scr_adPCA$cond == "nov",-c(1:4)],
                 avr_scr_chPCA[avr_scr_chPCA$cond == "nov",-c(1:4)])


rownames(Rloadings) <- rownames(Rtopo_sta) <- rownames(Rtopo_nov) <- paste0("adFA", 1:nrow(Rloadings))
colnames(Rloadings) <- colnames(Rtopo_sta) <- colnames(Rtopo_nov) <- paste0("chFA_", 1:ncol(Rloadings))


########inspection function


sort_similarities <- function(similarities){
  # Report maximum correlation per row to see if there is 
  # a match for a factor.
  similarities_sorted <- t(apply(round(similarities,2), 1, sort, decreasing = TRUE))
  
  # Which child factor is most similar to each adult factor?
  whichFactor <- t(apply(similarities, 1, order, decreasing = TRUE))
  
  out <- matrix(paste0("chF", whichFactor,"_r:", similarities_sorted), nrow = nrow(similarities),
         ncol = ncol(similarities))
  
  rownames(out) <- rownames(similarities)
  
  out
}


#### CONVENIENCE FUNCTIONS


# In these matrices, the rows still represent Factors from the adult PCA
# but the similarities in the columns were sorted
# and the respective factor was named.
# Example:
#        [,1]           [,2]           [,3]                     
# adFA1  "chF1_r:0.95"  "chF11_r:0.42" "chF13_r:0.39"
# This means that adult Factor 1 is most similar to child Factor 1 (followed by 11 and 13),
# with correlations of 0.95, 0.42 and 0.39

sort_similarities(similarities = Rloadings)
sort_similarities(similarities = Rtopo_sta)
sort_similarities(similarities = Rtopo_nov)


#### EXPORT TO EXCEL AS TABLES
## Format matrices for export
Rloadings <- apply(Rloadings, MARGIN = 1:2, function(x) sprintf("%.2f",x))
Rtopo_sta <- apply(Rtopo_sta, MARGIN = 1:2, function(x) sprintf("%.2f",x))
Rtopo_nov<- apply(Rtopo_nov, MARGIN = 1:2, function(x) sprintf("%.2f",x))


# Export to Excel-Files
write.xlsx(x = Rloadings[1:6,1:6], file = "Z:/Bureau/data_analysis/data/Tables.xlsx", sheetName = "Table1")
write.xlsx(x = Rtopo_sta[1:6,1:6], file = "Z:/Bureau/data_analysis/data/Tables.xlsx", sheetName = "Table2", append = TRUE)
write.xlsx(x = Rtopo_nov[1:6,1:6], file = "Z:/Bureau/data_analysis/data/Tables.xlsx", sheetName = "Table3", append = TRUE)

if(!require(ggplot2)) install.packages("ggplot2")
if(!require(RColorBrewer)) install.packages("RColorBrewer")
if(!require(reshape)) install.packages("reshape")
if(!require(xlsx)) install.packages("xlsx")
if(!require(remotes)) install.packages("remotes")
if(!require(R.matlab)) install.packages("R.matlab")
if(!require(eegUtils)) remotes::install_github("craddm/eegUtils", ref = "v0.6.1")
if(!require(dplyr)) install.packages("dplyr")
if(!require(grid)) install.packages("grid")
if(!require(gridExtra)) install.packages("gridExtra")

library(grid)
library(gridExtra)
library(dplyr)
library(ggplot2)
library(RColorBrewer)
library(reshape)
library(xlsx)
library(eegUtils)
library(R.matlab)

gavr2long <- function(allGAVRs, iEl, iGroup, times){
  # Get grandaverages to plot 
  iGAVR <- allGAVRS[allGAVRS$chan == iEl & 
                      allGAVRS$group == iGroup, -c(4)]
  iGAVR$cond <- as.character(iGAVR$cond)
  iGAVR$chan <- as.character(iGAVR$cond)
  
  iGAVR[3,] <- c("nov - sta", iEl, iGroup, iGAVR[2,-c(1:3)] - iGAVR[1,-c(1:3)])
  
  # Reshape for plotting
  iGAVR <- melt(iGAVR, variable_name = "times")
  iGAVR$cond <- factor(iGAVR$cond, levels = c("sta", "nov", "nov - sta"))
  levels(iGAVR$times) <- times
  iGAVR$times <- as.numeric(as.character(iGAVR$times))
  
  return(iGAVR)
}


makeERPest  <- function(iScores, iPCA, iFactor, iEl, times){
  iERP_est <- t(apply(iScores, MARGIN = 1, function(x){
    iPCA$rotFit$loadings[, iFactor] * as.numeric(x[iFactor])
  }))
  
  iERP_est <- cbind(iScores[,1:2], iERP_est)
  
  iEst <- iERP_est[iERP_est$chan == iEl,]  
  iEst$cond <- as.character(iEst$cond)
  iEst$chan <- as.character(iEst$cond)
  iEst[3,] <- c("nov - sta", iEl, iEst[2,-c(1:2)] - iEst[1,-c(1:2)])
  
  # Reshape for plotting
  iEst <- melt(iEst, variable_name = "times")
  iEst$cond <- factor(iEst$cond, levels = c("sta", "nov", "nov - sta"))
  levels(iEst$times) <- times
  iEst$times <- as.numeric(as.character(iEst$times))
  iEst
  
}


pca2eeg <- function(iPCA, allAVRs, iGroup, iFactor){
  signals <- lapply(c("sta", "nov"), function(iCond) {
    iSignals <- matrix(iPCA$average_scores[iPCA$average_scores$cond == iCond, iFactor] * max(iPCA$rotFit$loadings[,iFactor]),
                       ncol = dim(allAVRs[[iGroup]][[iCond]]$signals)[2], 
                       nrow = dim(allAVRs[[iGroup]][[iCond]]$signals)[1], 
                       byrow = TRUE)
    
    
    # We need to convert this to the tibble_format used in eegUtils:
    colnames(iSignals) <- names(allAVRs[[iGroup]][[iCond]]$signals)
    
    iSignals
  })
  names(signals) <- c("sta", "nov")
  
  signals[["nov - sta"]] <- signals$nov - signals$sta
  
  signals
}

load("Z:/Bureau/data_analysis/data/rotfit_ty13_geomin0.01.Rdata",  temp_env <- new.env())
load("Z:/Bureau/data_analysis/data/rotfit_at23_geomin0.01.Rdata",  temp_env <- new.env())

write.csv(scores, file = "Z:/Bureau/data_analysis/data/final_scores.csv")